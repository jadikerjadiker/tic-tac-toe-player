Given that the other player went first in the top corner

The trainingMode 'error' was defined by:
#figure out if the training is done
if modeType=="error":
    totalError = np.zeros((len(batch[0][1]), 1))
    #add up the error among all the training examples
    for trainingExample in batch:
        totalError+=self.getFinalError(*trainingExample)
    #if the average error is low enough, return
    if np.sum(totalError)/totalError.shape[0]<=modeValue:
        return
        
Anyhow, that's how these were trained. And somehow I got encouraging results.
The method would have been called "avgSum" with the naming scheme described in "Thinking"


sky = LearningNet1([9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .001))
[0, 3, 12, 3, 12, 5, 5, 4, 6] = 50

sky = LearningNet1([9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 5, trainingMode = ('error', .0001))
[0, 13, 16, 8, 17, 7, 16, 10, 13] = 100

sky = LearningNet1([9, 9, 9, 9], trainingGameAmt = 10000, learningRate = 5, trainingMode = ('error', .0001))
[0, 1, 0, 0, 0, 0, 2, 1, 1] = 5

sky = LearningNet1([9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 6, 17, 13, 21, 9, 15, 8, 11] = 100

sky = LearningNet1([9, 100, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 3, 21, 7, 20, 11, 11, 14, 13] = 100

sky = LearningNet1([9, 300, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 8, 18, 11, 21, 10, 12, 10, 10] = 100

sky = LearningNet1([9, 3000, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 2, 5, 2, 4, 2, 7, 0, 8] = 30
[0, 3, 9, 13, 16, 5, 13, 4, 12] = 75
[0, 10, 14, 15, 19, 7, 14, 6, 15] = 100

sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 5, 8, 6, 14, 5, 4, 3, 5] = 50
[0, 6, 12, 7, 19, 9, 9, 4, 9] = 75
[0, 12, 15, 8, 22, 10, 11, 7, 15] = 100

sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .1))
[0, 2, 5, 2, 4, 3, 7, 2, 5] = 30

sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 100, trainingMode = ('iter', 5))
[0, 8, 7, 5, 8, 0, 4, 0, 8] = 40
[0, 10, 9, 5, 10, 2, 4, 2, 8] = 50
[0, 13, 13, 7, 14, 6, 7, 4, 11] = 75
[0, 19, 18, 9, 16, 7, 10, 7, 14] = 100


Given that the other player went in the middle
sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[12, 7, 8, 10, 0, 4, 8, 7, 14] = 70


Given that nobody has gone yet
sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[2, 1, 2, 2, 6, 3, 4, 7, 3] = 30
[3, 2, 3, 4, 14, 4, 7, 8, 5] = 50

Playing 1 random [9, 9, 9, 9] net against random players 100 times:
Lows: [560, 90, 233] #lowest values out of all 100 tests
Highs: [639, 142, 319] #highest values out of all 100 tests
Ranges: [79, 52, 86] #the range between the highest and lowest out of all 100 tests
Averages: [607.52, 114.06, 278.42] #the averages of all 100 tests

Lows: [362, 147, 413]
Highs: [414, 203, 477]
Ranges: [52, 56, 64]
Averages: [385.2, 176.98, 437.82]

Lows: [296, 227, 359]
Highs: [373, 272, 454]
Ranges: [77, 45, 95]
Averages: [338.76, 248.56, 412.68]


net = LearningNet1([9, 9, 9, 9], learningRate = 1, trainingMode = ('error', .00000001), trainingGameAmt = 1000)
Lows: [379, 51, 489]
Highs: [438, 86, 558]
Ranges: [59, 35, 69]
Averages: [408.22, 67.84, 523.94]


net = LearningNet1([9, 9, 9, 9], learningRate = 1, trainingMode = ('error', .001), trainingGameAmt = 1000)
Lows: [284, 61, 552]
Highs: [364, 92, 625]
Ranges: [80, 31, 73]
Averages: [330.04, 77.38, 592.58]

Lows: [340, 62, 504]
Highs: [404, 114, 575]
Ranges: [64, 52, 71]
Averages: [378.44, 82.92, 538.64]

net = LearningNet3([9, 9, 9, 9], learningRate = .1, trainingMode = ('avgAvg', .15), examplesPerBatch = 1)
net.go(gamesPerRound = 500, rounds = 2, comment = False)
Lows: [304, 50, 573]
Highs: [368, 84, 628]
Ranges: [64, 34, 55]
Averages: [334.56, 67.08, 598.36]

Lows: [140, 45, 757]
Highs: [181, 71, 810]
Ranges: [41, 26, 53]
Averages: [161.48, 57.3, 781.22]

Lows: [340, 107, 440]
Highs: [427, 162, 508]
Ranges: [87, 55, 68]
Averages: [389.58, 134.34, 476.08]

net = LearningNet3([9, 9, 9, 9], learningRate = .1, trainingMode = ('avgAvg', .05), examplesPerBatch = 1)
net.go(gamesPerRound = 500, rounds = 2, comment = True)
Lows: [263, 91, 570]
Highs: [330, 128, 625]
Ranges: [67, 37, 55]
Averages: [295.94, 113.28, 590.78]

Lows: [273, 4, 645]
Highs: [341, 22, 714]
Ranges: [68, 18, 69]
Averages: [309.66, 12.32, 678.02]

Lows: [334, 122, 429]
Highs: [406, 180, 511]
Ranges: [72, 58, 82]
Averages: [366.42, 155.44, 478.14]

Lows: [405, 186, 315]
Highs: [468, 248, 377]
Ranges: [63, 62, 62]
Averages: [435.96, 218.76, 345.28]

Lows: [234, 43, 642]
Highs: [297, 83, 707]
Ranges: [63, 40, 65]
Averages: [264.28, 64.64, 671.08]

Lows: [173, 116, 623]
Highs: [236, 149, 692]
Ranges: [63, 33, 69]
Averages: [210.44, 134.94, 654.62]

net = LearningNet3([9, 9, 9, 9], learningRate = .0001, trainingMode = ('avgAvg', .198), examplesPerBatch = None)
net.go(gamesPerRound = 1000, rounds = 1, comment = 3)
Lows: [420, 124, 373]
Highs: [476, 172, 434]
Ranges: [56, 48, 61]
Averages: [445.72, 149.7, 404.58]

With the policy player, these are the types of results I'm getting:
p = PolicyPlayer(exploreRate = .1, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [5, 62, 878]
Highs: [20, 107, 925]
Ranges: [15, 45, 47]
Averages: [13.1, 83.58, 903.32]

p = PolicyPlayer(exploreRate = .15, learningRate = .5, rewards = [0, 1, 2])
gamesToPlay = 100000
playAgainst = 'random'
Lows: [4, 49, 902]
Highs: [18, 87, 942]
Ranges: [14, 38, 40]
Averages: [9.22, 66.2, 924.58]

So this one was an odd combo, but it gave great results. Still PolicyPlayer
#This is the actual code for it
p = PolicyPlayer(exploreRate = .15, learningRate = .5, rewards = [0, .5, 3])
gamesToPlay = 250000
playAgainst = p
for i in range(gamesToPlay):
    useful.printPercent(i, gamesToPlay, 5, 1)
    g = tttg.play(who = (playAgainst, p))
    p.update()
    p.curGameInfo[2]*=-1
    p.update()

print("Second training!")
playAgainst = 'random'
p.rewards = [-10, 1, 1]
for i in range(gamesToPlay):
    useful.printPercent(i, gamesToPlay, 5, 1)
    g = tttg.play(who = (playAgainst, p))
    p.update()
Lows: [0, 132, 802]
Highs: [5, 197, 867]
Ranges: [5, 65, 65]
Averages: [1.56, 170.5, 827.94]

Doing the same thing the next time yielded:
Lows: [3, 135, 792]
Highs: [12, 200, 862]
Ranges: [9, 65, 70]
Averages: [7.74, 170.68, 821.58]

Just punishing versus random didn't seem to do quite as well:
p = PolicyPlayer(exploreRate = .15, learningRate = .5, rewards = [-10, 1, 3])
gamesToPlay = 250000
playAgainst = 'random'
Lows: [8, 49, 888]
Highs: [24, 95, 932]
Ranges: [16, 46, 44]
Averages: [17.06, 72.38, 910.56]



The new policy player is giving results like this:
(The previous results that were here were incorrect
...because it reverted to an old version without the exploring implemented)


p = PolicyPlayer(exploreRate = .1, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [24, 37, 886]
Highs: [53, 67, 928]
Ranges: [29, 30, 42]
Averages: [36.14, 53.6, 910.26]

p = PolicyPlayer(exploreRate = 0, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [3, 191, 730]
Highs: [13, 262, 796]
Ranges: [10, 71, 66]
Averages: [6.8, 226.76, 766.44]

So this looks like something is wrong with exploring. Exploring should cause it to be better, not worse.
p = PolicyPlayer(exploreRate = .05, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [29, 46, 871]
Highs: [59, 82, 912]
Ranges: [30, 36, 41]
Averages: [41.72, 65.2, 893.08]

p = PolicyPlayer(exploreRate = 0, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [3, 174, 767]
Highs: [17, 225, 813]
Ranges: [14, 51, 46]
Averages: [8.98, 201.9, 789.12]

p = PolicyPlayer(exploreRate = .0001, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [7, 174, 748]
Highs: [26, 237, 817]
Ranges: [19, 63, 69]
Averages: [16.6, 202.64, 780.76]

p = PolicyPlayer(exploreRate = .0000, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [9, 155, 773]
Highs: [24, 211, 827]
Ranges: [15, 56, 54]
Averages: [16.04, 183.96, 800.0]

p = PolicyPlayer(exploreRate = 0, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [8, 166, 765]
Highs: [24, 219, 818]
Ranges: [16, 53, 53]
Averages: [14.84, 192.44, 792.72]

p = PolicyPlayer(exploreRate = .001, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [14, 160, 757]
Highs: [34, 215, 819]
Ranges: [20, 55, 62]
Averages: [24.58, 188.9, 786.52]

p = PolicyPlayer(exploreRate = .001, learningRate = .5, rewards = [-1, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [1, 174, 769]
Highs: [14, 220, 821]
Ranges: [13, 46, 52]
Averages: [5.76, 200.72, 793.52]

p = PolicyPlayer(exploreRate = .001, learningRate = .5, rewards = [-1, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [0, 160, 779]
Highs: [7, 217, 837]
Ranges: [7, 57, 58]
Averages: [2.76, 185.0, 812.24]

p = PolicyPlayer(exploreRate = .001, learningRate = .5, rewards = [-1, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [0, 119, 822]
Highs: [7, 177, 878]
Ranges: [7, 58, 56]
Averages: [3.56, 147.48, 848.96]



The new policy player is giving results like this:
(The previous results that were here were incorrect
...because it reverted to an old version without the exploring implemented)


p = PolicyPlayer(exploreRate = .1, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [24, 37, 886]
Highs: [53, 67, 928]
Ranges: [29, 30, 42]
Averages: [36.14, 53.6, 910.26]

p = PolicyPlayer(exploreRate = 0, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [3, 191, 730]
Highs: [13, 262, 796]
Ranges: [10, 71, 66]
Averages: [6.8, 226.76, 766.44]

So this looks like something is wrong with exploring. Exploring should cause it to be better, not worse.
p = PolicyPlayer(exploreRate = .05, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [29, 46, 871]
Highs: [59, 82, 912]
Ranges: [30, 36, 41]
Averages: [41.72, 65.2, 893.08]

p = PolicyPlayer(exploreRate = 0, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [3, 174, 767]
Highs: [17, 225, 813]
Ranges: [14, 51, 46]
Averages: [8.98, 201.9, 789.12]

p = PolicyPlayer(exploreRate = .0001, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [7, 174, 748]
Highs: [26, 237, 817]
Ranges: [19, 63, 69]
Averages: [16.6, 202.64, 780.76]

p = PolicyPlayer(exploreRate = .0000, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [9, 155, 773]
Highs: [24, 211, 827]
Ranges: [15, 56, 54]
Averages: [16.04, 183.96, 800.0]

p = PolicyPlayer(exploreRate = 0, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [8, 166, 765]
Highs: [24, 219, 818]
Ranges: [16, 53, 53]
Averages: [14.84, 192.44, 792.72]

p = PolicyPlayer(exploreRate = .001, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [14, 160, 757]
Highs: [34, 215, 819]
Ranges: [20, 55, 62]
Averages: [24.58, 188.9, 786.52]

p = PolicyPlayer(exploreRate = .001, learningRate = .5, rewards = [-1, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [1, 174, 769]
Highs: [14, 220, 821]
Ranges: [13, 46, 52]
Averages: [5.76, 200.72, 793.52]

p = PolicyPlayer(exploreRate = .001, learningRate = .5, rewards = [-1, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [0, 160, 779]
Highs: [7, 217, 837]
Ranges: [7, 57, 58]
Averages: [2.76, 185.0, 812.24]

p = PolicyPlayer(exploreRate = .001, learningRate = .5, rewards = [-1, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [0, 119, 822]
Highs: [7, 177, 878]
Ranges: [7, 58, 56]
Averages: [3.56, 147.48, 848.96]


p = PolicyPlayer(exploreRate = .2, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 1000
playAgainst = 'random'
Lows: [87, 175, 657]
Highs: [141, 220, 709]
Ranges: [54, 45, 52]
Averages: [119.52, 196.2, 684.28]

Lows: [69, 150, 705]
Highs: [117, 208, 761]
Ranges: [48, 58, 56]
Averages: [89.9, 177.72, 732.38]

Lows: [75, 117, 716]
Highs: [121, 176, 781]
Ranges: [46, 59, 65]
Averages: [104.7, 144.34, 750.96]

exploreRate = .1

Lows: [69, 88, 778]
Highs: [116, 127, 832]
Ranges: [47, 39, 54]
Averages: [91.8, 104.42, 803.78]

Lows: [57, 77, 803]
Highs: [96, 115, 854]
Ranges: [39, 38, 51]
Averages: [77.34, 91.52, 831.14]

Lows: [89, 88, 739]
Highs: [135, 131, 821]
Ranges: [46, 43, 82]
Averages: [107.66, 113.94, 778.4]

exploreRate = .2

Lows: [82, 48, 805]
Highs: [122, 87, 856]
Ranges: [40, 39, 51]
Averages: [101.58, 68.48, 829.94]

Lows: [105, 77, 758]
Highs: [147, 118, 806]
Ranges: [42, 41, 48]
Averages: [127.46, 95.48, 777.06]

Lows: [132, 69, 735]
Highs: [177, 103, 796]
Ranges: [45, 34, 61]
Averages: [148.84, 84.98, 766.18]


#Generally seems to take 5 tries to get
exploreRate, learningRate, rewards = (0, .5, [-10, 1, 10])
p = PolicyPlayer(exploreRate = exploreRate, learningRate = learningRate, rewards = rewards)
playAgainst = 'random'
gamesToPlay = 80000
Lows: [0, 165, 767]
Highs: [0, 233, 835]
Ranges: [0, 68, 68]
Averages: [0.0, 203.2, 796.8]

#Took about 5 tries to get
exploreRate, learningRate, rewards = (0, .5, [-20, 1, 20])
p = PolicyPlayer(exploreRate = exploreRate, learningRate = learningRate, rewards = rewards)
playAgainst = 'random'
gamesToPlay = 100000
Lows: [0, 165, 757]
Highs: [0, 243, 835]
Ranges: [0, 78, 78]
Averages: [0.0, 214.94, 785.06]

#Took about 5 tries to get
exploreRate, learningRate, rewards = (0, .5, [-3, 0, 5])
p = PolicyPlayer(exploreRate = exploreRate, learningRate = learningRate, rewards = rewards)
playAgainst = 'random'
gamesToPlay = 150000
Lows: [0, 55, 910]
Highs: [0, 90, 945]
Ranges: [0, 35, 35]
Averages: [0.0, 69.44, 930.56]