Given that the other player went first in the top corner

The trainingMode 'error' was defined by:
#figure out if the training is done
if modeType=="error":
    totalError = np.zeros((len(batch[0][1]), 1))
    #add up the error among all the training examples
    for trainingExample in batch:
        totalError+=self.getFinalError(*trainingExample)
    #if the average error is low enough, return
    if np.sum(totalError)/totalError.shape[0]<=modeValue:
        return
        
Anyhow, that's how these were trained. And somehow I got encouraging results.
The method would have been called "avgSum" with the naming scheme described in "Thinking"


sky = LearningNet1([9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .001))
[0, 3, 12, 3, 12, 5, 5, 4, 6] = 50

sky = LearningNet1([9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 5, trainingMode = ('error', .0001))
[0, 13, 16, 8, 17, 7, 16, 10, 13] = 100

sky = LearningNet1([9, 9, 9, 9], trainingGameAmt = 10000, learningRate = 5, trainingMode = ('error', .0001))
[0, 1, 0, 0, 0, 0, 2, 1, 1] = 5

sky = LearningNet1([9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 6, 17, 13, 21, 9, 15, 8, 11] = 100

sky = LearningNet1([9, 100, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 3, 21, 7, 20, 11, 11, 14, 13] = 100

sky = LearningNet1([9, 300, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 8, 18, 11, 21, 10, 12, 10, 10] = 100

sky = LearningNet1([9, 3000, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 2, 5, 2, 4, 2, 7, 0, 8] = 30
[0, 3, 9, 13, 16, 5, 13, 4, 12] = 75
[0, 10, 14, 15, 19, 7, 14, 6, 15] = 100

sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[0, 5, 8, 6, 14, 5, 4, 3, 5] = 50
[0, 6, 12, 7, 19, 9, 9, 4, 9] = 75
[0, 12, 15, 8, 22, 10, 11, 7, 15] = 100

sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .1))
[0, 2, 5, 2, 4, 3, 7, 2, 5] = 30

sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 100, trainingMode = ('iter', 5))
[0, 8, 7, 5, 8, 0, 4, 0, 8] = 40
[0, 10, 9, 5, 10, 2, 4, 2, 8] = 50
[0, 13, 13, 7, 14, 6, 7, 4, 11] = 75
[0, 19, 18, 9, 16, 7, 10, 7, 14] = 100


Given that the other player went in the middle
sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[12, 7, 8, 10, 0, 4, 8, 7, 14] = 70


Given that nobody has gone yet
sky = LearningNet1([9, 9, 9, 9, 9], trainingGameAmt = 1000, learningRate = 1, trainingMode = ('error', .01))
[2, 1, 2, 2, 6, 3, 4, 7, 3] = 30
[3, 2, 3, 4, 14, 4, 7, 8, 5] = 50

Playing 1 random [9, 9, 9, 9] net against random players 100 times:
Lows: [560, 90, 233] #lowest values out of all 100 tests
Highs: [639, 142, 319] #highest values out of all 100 tests
Ranges: [79, 52, 86] #the range between the highest and lowest out of all 100 tests
Averages: [607.52, 114.06, 278.42] #the averages of all 100 tests

Lows: [362, 147, 413]
Highs: [414, 203, 477]
Ranges: [52, 56, 64]
Averages: [385.2, 176.98, 437.82]

Lows: [296, 227, 359]
Highs: [373, 272, 454]
Ranges: [77, 45, 95]
Averages: [338.76, 248.56, 412.68]


net = LearningNet1([9, 9, 9, 9], learningRate = 1, trainingMode = ('error', .00000001), trainingGameAmt = 1000)
Lows: [379, 51, 489]
Highs: [438, 86, 558]
Ranges: [59, 35, 69]
Averages: [408.22, 67.84, 523.94]


net = LearningNet1([9, 9, 9, 9], learningRate = 1, trainingMode = ('error', .001), trainingGameAmt = 1000)
Lows: [284, 61, 552]
Highs: [364, 92, 625]
Ranges: [80, 31, 73]
Averages: [330.04, 77.38, 592.58]

Lows: [340, 62, 504]
Highs: [404, 114, 575]
Ranges: [64, 52, 71]
Averages: [378.44, 82.92, 538.64]

net = LearningNet3([9, 9, 9, 9], learningRate = .1, trainingMode = ('avgAvg', .15), examplesPerBatch = 1)
net.go(gamesPerRound = 500, rounds = 2, comment = False)
Lows: [304, 50, 573]
Highs: [368, 84, 628]
Ranges: [64, 34, 55]
Averages: [334.56, 67.08, 598.36]

Lows: [140, 45, 757]
Highs: [181, 71, 810]
Ranges: [41, 26, 53]
Averages: [161.48, 57.3, 781.22]

Lows: [340, 107, 440]
Highs: [427, 162, 508]
Ranges: [87, 55, 68]
Averages: [389.58, 134.34, 476.08]

net = LearningNet3([9, 9, 9, 9], learningRate = .1, trainingMode = ('avgAvg', .05), examplesPerBatch = 1)
net.go(gamesPerRound = 500, rounds = 2, comment = True)
Lows: [263, 91, 570]
Highs: [330, 128, 625]
Ranges: [67, 37, 55]
Averages: [295.94, 113.28, 590.78]

Lows: [273, 4, 645]
Highs: [341, 22, 714]
Ranges: [68, 18, 69]
Averages: [309.66, 12.32, 678.02]

Lows: [334, 122, 429]
Highs: [406, 180, 511]
Ranges: [72, 58, 82]
Averages: [366.42, 155.44, 478.14]

Lows: [405, 186, 315]
Highs: [468, 248, 377]
Ranges: [63, 62, 62]
Averages: [435.96, 218.76, 345.28]

Lows: [234, 43, 642]
Highs: [297, 83, 707]
Ranges: [63, 40, 65]
Averages: [264.28, 64.64, 671.08]

Lows: [173, 116, 623]
Highs: [236, 149, 692]
Ranges: [63, 33, 69]
Averages: [210.44, 134.94, 654.62]

net = LearningNet3([9, 9, 9, 9], learningRate = .0001, trainingMode = ('avgAvg', .198), examplesPerBatch = None)
net.go(gamesPerRound = 1000, rounds = 1, comment = 3)
Lows: [420, 124, 373]
Highs: [476, 172, 434]
Ranges: [56, 48, 61]
Averages: [445.72, 149.7, 404.58]

With the policy player, these are the types of results I'm getting:
p = PolicyPlayer(exploreRate = .1, learningRate = .5, rewards = [0, 1, 3])
gamesToPlay = 80000
playAgainst = 'random'
Lows: [5, 62, 878]
Highs: [20, 107, 925]
Ranges: [15, 45, 47]
Averages: [13.1, 83.58, 903.32]

p = PolicyPlayer(exploreRate = .15, learningRate = .5, rewards = [0, 1, 2])
gamesToPlay = 100000
playAgainst = 'random'
Lows: [4, 49, 902]
Highs: [18, 87, 942]
Ranges: [14, 38, 40]
Averages: [9.22, 66.2, 924.58]

So this one was an odd combo, but it gave great results. Still PolicyPlayer
#This is the actual code for it
p = PolicyPlayer(exploreRate = .15, learningRate = .5, rewards = [0, .5, 3])
gamesToPlay = 250000
playAgainst = p
for i in range(gamesToPlay):
    useful.printPercent(i, gamesToPlay, 5, 1)
    g = tttg.play(who = (playAgainst, p))
    p.update()
    p.curGameInfo[2]*=-1
    p.update()

print("Second training!")
playAgainst = 'random'
p.rewards = [-10, 1, 1]
for i in range(gamesToPlay):
    useful.printPercent(i, gamesToPlay, 5, 1)
    g = tttg.play(who = (playAgainst, p))
    p.update()
Lows: [0, 132, 802]
Highs: [5, 197, 867]
Ranges: [5, 65, 65]
Averages: [1.56, 170.5, 827.94]

Doing the same thing the next time yielded:
Lows: [3, 135, 792]
Highs: [12, 200, 862]
Ranges: [9, 65, 70]
Averages: [7.74, 170.68, 821.58]

Just punishing versus random didn't seem to do quite as well:
p = PolicyPlayer(exploreRate = .15, learningRate = .5, rewards = [-10, 1, 3])
gamesToPlay = 250000
playAgainst = 'random'
Lows: [8, 49, 888]
Highs: [24, 95, 932]
Ranges: [16, 46, 44]
Averages: [17.06, 72.38, 910.56]